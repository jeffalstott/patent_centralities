{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_directory = '../data/'\n",
    "# randomized_control = False\n",
    "# # randomization_id = 1\n",
    "# class_system = 'USPC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pylab import *\n",
    "import gc\n",
    "from tqdm import *\n",
    "import igraph as igraph\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Functions\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_patent_citation_graph(PATENT_INFO, citations):\n",
    "    ti = time.time()\n",
    "    print('create igraph object and populate it with patent attributes')\n",
    "    G = igraph.Graph(directed=True)\n",
    "\n",
    "    ##### add nodes ####\n",
    "    print('step 1: add nodes')\n",
    "    num_nodes = shape(PATENT_INFO)[0]\n",
    "    G.add_vertices(num_nodes)\n",
    "\n",
    "    #### add nodes attributes ####\n",
    "    print('step 2: add node attributes')\n",
    "    for x in PATENT_INFO.columns:\n",
    "        G.vs[x] = array(PATENT_INFO[x])\n",
    "        #if x == 'patent_number': continue\n",
    "        #else: G.vs[x] = array(PATENT_INFO[x])\n",
    "\n",
    "    #### add edges ####\n",
    "    print('step 3: add edges')\n",
    "    # create a series with 'patent_number' as index and ID from zero to N as values. \n",
    "    # We will then use it to translate the edgelist from patent numbers to patent IDs. We will use 'map' for that\n",
    "    PATENT_INFO['ID'] = range(shape(PATENT_INFO)[0])\n",
    "    series_patent_ids = PATENT_INFO[['ID','patent_number']].set_index('patent_number').ix[:,0] #s = df.ix[:,0] <-- this create a series out of a dataframe (see: http://stackoverflow.com/questions/15360925/how-to-get-the-first-column-of-a-pandas-dataframe-as-a-series)\n",
    "    # Now map patent numbers to IDs in our edgelist\n",
    "    citations['citing_numerical_id'] = citations['Citing_Patent'].map(series_patent_ids) # see: http://stackoverflow.com/questions/25653652/how-to-replace-efficiently-values-on-a-pandas-dataframe\n",
    "    citations['cited_numerical_id'] = citations['Cited_Patent'].map(series_patent_ids)\n",
    "    \n",
    "\n",
    "    # create edgelist with IDs, not patent_numbers (to understand why see:http://igraph.org/python/doc/tutorial/tutorial.html)\n",
    "    # incidentally, the index of the edgelist dataframe is the edge ID in the igraph object (unless we later remove nodes or edges)\n",
    "    edgelist = citations[['cited_numerical_id','citing_numerical_id']]\n",
    "\n",
    "    # now populate our graph G with edges. We will then be able to select subgraphs based on nodes (or edges) attributes\n",
    "    G.add_edges(edgelist.to_records(index=False))\n",
    "\n",
    "    del edgelist\n",
    "    gc.collect()\n",
    "\n",
    "    tf = time.time()\n",
    "    time_length = (tf-ti)/60 # unit = minutes\n",
    "    print(\"Done! Elapsed time: %f minutes\" %time_length) # takes 8.2 minutes to run\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topologically_sort_graph(G):\n",
    "    if not G.is_dag():\n",
    "        print('Graph is not DAG. If you continue the topological sorting algorithm will be stuck in an endless search. Remove cycles before to continue!')\n",
    "        raise ValueError\n",
    "    layers = -1 * ones(G.vcount())\n",
    "    nodes_in_this_layer = where(array(G.indegree())==0)[0]\n",
    "    layer = 0\n",
    "\n",
    "    while nodes_in_this_layer.any():\n",
    "        layers[nodes_in_this_layer] = layer\n",
    "        layer += 1\n",
    "        nodes_in_this_layer = G.neighborhood(vertices=nodes_in_this_layer.tolist(), order=1, mode='OUT')\n",
    "        nodes_in_this_layer = unique([item for sublist in nodes_in_this_layer for item in sublist[1:]])\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def search_path_count_of_graph(G, mode='IN', layer_name='layer'):\n",
    "    layers = G.vs[layer_name]\n",
    "    if mode=='IN':\n",
    "        layer_values = arange(2,max(layers)+1)\n",
    "    elif mode=='OUT':\n",
    "        layer_values = arange(max(layers)-2, -1, -1)\n",
    "    count_paths = array(G.degree(mode=mode))\n",
    "    for layer in layer_values:\n",
    "        for n in where(layers==layer)[0]:\n",
    "            neighbors = G.neighbors(n, mode=mode)\n",
    "            if neighbors:\n",
    "            #Each node's count of incoming paths is the sum of its predecessors' count of incoming paths\n",
    "                count_paths[n] += sum(count_paths[array(neighbors)])\n",
    "    return count_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Define functions to generate random controls\n",
    "def randomize_citations(citations,\n",
    "                        patent_attributes):\n",
    "    citations_randomized = citations.copy()\n",
    "    not_same_year = citations_randomized['Year_Citing_Patent']!=citations_randomized['Year_Cited_Patent']\n",
    "    ### Take the same-class citations of every class and permute them.\n",
    "    print(\"Randomizing Same-Class Citations\")\n",
    "    same_class_ind = citations_randomized['Class_Citing_Patent']==citations_randomized['Class_Cited_Patent']\n",
    "    cross_class_ind = -same_class_ind \n",
    "    same_class_ind = same_class_ind & not_same_year\n",
    "    grouper = citations_randomized.ix[same_class_ind].groupby(['Year_Citing_Patent', \n",
    "                                                               'Year_Cited_Patent', \n",
    "                                                               'Class_Citing_Patent', \n",
    "                                                              ])[['Citing_Patent', \n",
    "                                                                  'Cited_Patent']]\n",
    "    print(\"%i groups\"%(len(grouper)))\n",
    "    print(\"%i groups that can be rewired\"%(sum(grouper.size()>1)))\n",
    "    g = grouper.apply(randomize_citations_helper)\n",
    "#     g.index = g.index.droplevel(['Year_Citing_Patent','Year_Cited_Patent','Class_Citing_Patent'])\n",
    "\n",
    "    citations_randomized.ix[same_class_ind, ['Citing_Patent', \n",
    "                                             'Cited_Patent']\n",
    "                            ] = g\n",
    "\n",
    "    ### Take the cross-class citations and permute them.\n",
    "    print(\"Randomizing Cross-Class Citations\")        \n",
    "    cross_class_ind = cross_class_ind & not_same_year\n",
    "    grouper = citations_randomized.ix[cross_class_ind].groupby(['Year_Citing_Patent', \n",
    "                                                               'Year_Cited_Patent', \n",
    "                                                              ])[['Citing_Patent', \n",
    "                                                                  'Cited_Patent']]\n",
    "    print(\"%i groups\"%(len(grouper)))\n",
    "    print(\"%i groups that can be rewired\"%(sum(grouper.size()>1)))\n",
    "    g = grouper.apply(randomize_citations_helper)\n",
    "#     g.index = g.index.droplevel(['Year_Citing_Patent','Year_Cited_Patent'])\n",
    "\n",
    "    citations_randomized.ix[cross_class_ind, ['Citing_Patent', \n",
    "                                             'Cited_Patent']\n",
    "                            ] = g\n",
    "    \n",
    "    ### Drop patent attributes (which are now inaccurate for both the citing and cited patent) and bring them in from patent_attributes\n",
    "    citations_randomized.drop(['Class_Citing_Patent', 'Class_Cited_Patent'], axis=1, inplace=True)\n",
    "#     citations_randomized = citations_randomized[['Citing_Patent', 'Cited_Patent', 'Same_Class']]\n",
    "\n",
    "    patent_attributes = patent_attributes[['patent_number', 'Class']].set_index('patent_number')\n",
    "    citations_randomized = citations_randomized.merge(patent_attributes, \n",
    "                    left_on='Citing_Patent', \n",
    "                    right_index=True,\n",
    "                    )\n",
    "\n",
    "    citations_randomized = citations_randomized.merge(patent_attributes, \n",
    "                    left_on='Cited_Patent', \n",
    "                    right_index=True,\n",
    "                    suffixes=('_Citing_Patent','_Cited_Patent'))\n",
    "    return citations_randomized\n",
    "\n",
    "\n",
    "def randomize_citations_helper(citing_cited):\n",
    "\n",
    "#     if all(citing_cited['Year_Citing_Patent']==citing_cited['Year_Cited_Patent']):\n",
    "#         return citing_cited[['Citing_Patent', 'Cited_Patent']]\n",
    "    n_Citing = citing_cited.Citing_Patent.nunique()\n",
    "    n_Cited = citing_cited.Cited_Patent.nunique()\n",
    "    \n",
    "    if n_Cited*n_Citing==citing_cited.shape[0]: #The graph is fully connected, and so can't be rewired\n",
    "        return citing_cited#[['Citing_Patent', 'Cited_Patent']]\n",
    "    \n",
    "#     Citing_lookup = pd.Series(index=citing_cited.Citing_Patent.unique(),\n",
    "#                               data=1+arange(n_Citing))\n",
    "#     Cited_lookup = pd.Series(index=citing_cited.Cited_Patent.unique(),\n",
    "#                              data=1+arange(n_Cited))\n",
    "#     input_to_Birewire = array([Citing_lookup.ix[citing_cited.Citing_Patent].values,\n",
    "#                                Cited_lookup.ix[citing_cited.Cited_Patent].values + n_Citing]).T\n",
    "    citing_lookup = citing_cited['Citing_Patent'].astype('category')\n",
    "    cited_lookup = citing_cited['Cited_Patent'].astype('category')\n",
    "    input_to_Birewire = array([citing_lookup.cat.codes.values.astype('uint64'),\n",
    "                               cited_lookup.cat.codes.values.astype('uint64') + n_Citing]).T+1\n",
    "#     citing_cited.Citing_Patent = Citing_lookup.ix[citing_cited.Citing_Patent].values\n",
    "#     citing_cited.Cited_Patent = Cited_lookup.ix[citing_cited.Cited_Patent].values\n",
    "#     citing_cited.Cited_Patent += n_Citing\n",
    "    import BiRewire\n",
    "    this_rewiring = BiRewire.Rewiring(data=input_to_Birewire,\n",
    "                               type_of_array='edgelist_b',\n",
    "                               type_of_graph='bipartite')\n",
    "    this_rewiring.rewire(verbose=0)   \n",
    "    z = this_rewiring.data_rewired-1\n",
    "\n",
    "\n",
    "#     Citing_lookup = pd.DataFrame(Citing_lookup).reset_index().set_index(0)\n",
    "#     Cited_lookup = pd.DataFrame(Cited_lookup).reset_index().set_index(0)\n",
    "#     citing_patents = Citing_lookup.ix[z[:,0]].values.flatten()\n",
    "#     cited_patents = Cited_lookup.ix[z[:,1]-n_Citing].values.flatten()\n",
    "    \n",
    "    citing_patents = citing_lookup.cat.categories.values[z[:,0]]\n",
    "    cited_patents = cited_lookup.cat.categories.values[z[:,1]-n_Citing]\n",
    "\n",
    "    rewired_output = pd.DataFrame(index=citing_cited.index,\n",
    "                                 columns=['Citing_Patent', 'Cited_Patent']\n",
    "                                  )\n",
    "    rewired_output['Citing_Patent'] = citing_patents\n",
    "    rewired_output['Cited_Patent'] = cited_patents\n",
    "    return rewired_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patents = pd.read_hdf(data_directory+'patents.h5', 'df')\n",
    "citations = pd.read_hdf(data_directory+'citations.h5', 'df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomize Citations\n",
    "===\n",
    "If requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if randomized_control:\n",
    "    ti = time.time()\n",
    "    citations = randomize_citations(citations, patents)\n",
    "    tf = time.time()\n",
    "    final_time_length = tf-ti\n",
    "    print('Done! Randomizing citations took: %f seconds' %final_time_length + '= %f minutes' %(final_time_length/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute SPNP\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create igraph object and populate it with patent attributes\n",
      "step 1: add nodes\n",
      "step 2: add node attributes\n",
      "step 3: add edges\n",
      "Done! Elapsed time: 9.279987 minutes\n",
      "Graph is DAG, you can continue!\n"
     ]
    }
   ],
   "source": [
    "G = create_patent_citation_graph(patents, citations)\n",
    "if not G.is_dag():\n",
    "    print('Graph is not DAG. If you continue the topological sorting algorithm will be stuck in an endless search. Remove cycles before to continue!')\n",
    "    raise ValueError\n",
    "print('Graph is DAG, you can continue!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 layers\n"
     ]
    }
   ],
   "source": [
    "layers = topologically_sort_graph(G)\n",
    "G.vs['layer'] = layers\n",
    "print(\"%i layers\"%max(layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_incoming_paths = array(search_path_count_of_graph(G, mode='IN'))\n",
    "G.vs['count_incoming_paths'] = count_incoming_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated to DF_node_SPNP_over_time: 2.161447 GBs\n",
      "1975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 1/41 [00:23<15:52, 23.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 2/41 [00:36<13:20, 20.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 3/41 [00:49<11:36, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|▉         | 4/41 [01:03<10:30, 17.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 5/41 [01:18<09:50, 16.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▍        | 6/41 [01:34<09:29, 16.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 7/41 [01:51<09:19, 16.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█▉        | 8/41 [02:08<09:09, 16.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 9/41 [02:26<09:05, 17.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 10/41 [02:46<09:14, 17.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 11/41 [03:08<09:29, 18.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 12/41 [03:29<09:35, 19.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 13/41 [03:53<09:46, 20.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 14/41 [04:19<10:03, 22.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 15/41 [04:47<10:26, 24.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▉      | 16/41 [05:16<10:39, 25.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████▏     | 17/41 [05:47<10:56, 27.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 18/41 [06:23<11:23, 29.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▋     | 19/41 [07:01<11:52, 32.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████▉     | 20/41 [07:42<12:16, 35.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████     | 21/41 [08:26<12:30, 37.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▎    | 22/41 [09:11<12:39, 39.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 23/41 [09:59<12:39, 42.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▊    | 24/41 [10:51<12:50, 45.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████    | 25/41 [11:49<13:05, 49.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 26/41 [13:01<13:57, 55.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 27/41 [14:26<15:04, 64.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 28/41 [15:58<15:47, 72.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████   | 29/41 [17:39<16:16, 81.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 30/41 [19:14<15:39, 85.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 31/41 [21:05<15:30, 93.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 32/41 [22:52<14:36, 97.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 33/41 [25:04<14:21, 107.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 34/41 [27:17<13:26, 115.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 35/41 [29:27<11:57, 119.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 36/41 [31:56<10:42, 128.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 37/41 [34:26<08:59, 134.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 38/41 [36:58<06:59, 139.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 39/41 [40:03<05:06, 153.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 40/41 [43:10<02:43, 163.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [46:08<00:00, 167.76s/it]\n"
     ]
    }
   ],
   "source": [
    "vc = pd.value_counts(G.vs['filing_year'])\n",
    "n_rows = vc.sort_index().cumsum().ix[1975:].sum()\n",
    "DF = pd.DataFrame(index=arange(n_rows), \n",
    "                  columns=[\"patent_number\",\n",
    "                           \"observation_year\",\n",
    "                          \"SPNP_count\"],\n",
    "#                   dtype='uint64'\n",
    "                 )\n",
    "size_in_GBs = (prod(DF.shape)*64)*1.25e-10\n",
    "print(\"Memory allocated to DF_node_SPNP_over_time: %f GBs\" %size_in_GBs)\n",
    "   \n",
    "year_list = arange(1975,max(G.vs['filing_year'])+1)\n",
    "this_year_data_start = 0\n",
    "for observation_year in tqdm(year_list):\n",
    "    print(observation_year)\n",
    "    patents_within_this_year = G.vs.select(filing_year_le=observation_year).indices\n",
    "    G_subgraph = G.subgraph(patents_within_this_year, \n",
    "                            implementation=\"auto\")\n",
    "    n_row = G_subgraph.vcount()\n",
    "\n",
    "    DF.ix[this_year_data_start:this_year_data_start+n_row-1, \n",
    "          'SPNP_count'] = (log(search_path_count_of_graph(G_subgraph, mode='OUT')+1) +\n",
    "                           log(count_incoming_paths[patents_within_this_year]+1)\n",
    "                           )\n",
    "    DF.ix[this_year_data_start:this_year_data_start+n_row-1, \n",
    "          'patent_number'] = G_subgraph.vs['patent_number']\n",
    "    DF.ix[this_year_data_start:this_year_data_start+n_row-1, \n",
    "          'observation_year'] = observation_year\n",
    "    del G_subgraph\n",
    "    gc.collect()\n",
    "    this_year_data_start += n_row\n",
    "\n",
    "DF['observation_year'] = DF['observation_year'].astype('uint16')\n",
    "DF['patent_number'] = DF['patent_number'].astype('uint32')\n",
    "DF['SPNP_count'] = DF['SPNP_count'].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report patents' centrality in t+2, t+3, t+5 and t+8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffrey_alstott/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/jeffrey_alstott/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "DF_patents = DF[DF['observation_year']==2015]\n",
    "DF_patents.drop(['observation_year'], axis=1, inplace=True)\n",
    "DF_patents['filing_year'] = G.vs['filing_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffrey_alstott/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/jeffrey_alstott/anaconda3/lib/python3.5/site-packages/pandas/core/indexing.py:465: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/jeffrey_alstott/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "100%|██████████| 4/4 [02:55<00:00, 45.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Reporting patent centrality after x years took: 176.424691 seconds= 2.940412 minutes\n"
     ]
    }
   ],
   "source": [
    "ti = time.time()\n",
    "# for patents filed AFTER 1975 report their centrality 3/5/8 years after filing\n",
    "# for patents filed BEFORE 1975 report their centrality in 1978/1980/1983. To quickly do this create a \"fake filing year\"\n",
    "DF_patents['fake_filing_year'] = DF_patents['filing_year']\n",
    "DF_patents.ix[DF_patents['filing_year']<1975, 'fake_filing_year']=1975\n",
    "\n",
    "for horizon in tqdm([2,3,5,8]):\n",
    "    DF_patents['filing_year+%i'%horizon] = DF_patents['fake_filing_year']+horizon\n",
    "\n",
    "    # merge to add SPNP after $horizon years from filing\n",
    "    DF_patents = pd.merge(DF_patents, DF, \n",
    "                          how='left', left_on=['patent_number','filing_year+%i'%horizon], \n",
    "                          right_on=['patent_number','observation_year'],\n",
    "                           suffixes=('', '_filing+%i'%horizon)\n",
    "                           )\n",
    "#                            sort=True,, copy=True, indicator=False)\n",
    "    del DF_patents['filing_year+%i'%horizon]\n",
    "    del DF_patents['observation_year']\n",
    "    \n",
    "    gc.collect()\n",
    "#     DF_patents.rename(columns={\"SPNP_count\": 'SPNP_count_t+%i'%horizon}, inplace=True)\n",
    "\n",
    "del DF_patents['fake_filing_year']\n",
    "DF_patents.rename(columns={'SPNP_count': 'SPNP_count_2015'}, inplace=True)\n",
    "tf = time.time()\n",
    "final_time_length = tf-ti\n",
    "print('Done! Reporting patent centrality after x years took: %f seconds' %final_time_length + '= %f minutes' %(final_time_length/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute centrality of cited patents in t-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "citations = pd.merge(citations, DF,\n",
    "                      how='left', on=None, left_on=['Cited_Patent','Year_Citing_Patent'], \n",
    "                      right_on=['patent_number','observation_year'],\n",
    "                      )\n",
    "del citations['patent_number']\n",
    "del citations['observation_year']\n",
    "gc.collect()\n",
    "citations.rename(columns={\"SPNP_count\": 'SPNP_count_cited_year_of_citation'}, inplace=True)\n",
    "\n",
    "citations_grouped_by_citing = citations[['SPNP_count_cited_year_of_citation',\n",
    "                                          'Citing_Patent']].groupby(['Citing_Patent']).agg(['mean',\n",
    "                                                                                            'std'])\n",
    "\n",
    "DF_patents = pd.merge(DF_patents, citations_grouped_by_citing['SPNP_count_cited_year_of_citation'], \n",
    "                      how='left', left_on='patent_number', \n",
    "                      right_index=True)\n",
    "\n",
    "DF_patents.rename(columns={\"mean\": 'meanSPNPcited',\n",
    "                          \"std\": 'stdSPNPcited'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some patents are cited the same year they were filed. For these patents we have no information on their SPNP the year before they were cited. We need to compute their SPNP the moment before they were cited. This is done by multiplying the number of incoming paths of the cited patent by the number of citations received in the year they were filed. This is only an approximation of the number of their outgoing paths, but it is not a bad one because they are unlikely to be cited by other patents granted in the same year that have received citations themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "citations['Year_Citing_Patent-1'] = citations['Year_Citing_Patent'] - 1\n",
    "\n",
    "citations = pd.merge(citations, DF,\n",
    "                      how='left', on=None, left_on=['Cited_Patent','Year_Citing_Patent-1'], \n",
    "                      right_on=['patent_number','observation_year'],\n",
    "                      )\n",
    "del citations['patent_number']\n",
    "del citations['observation_year']\n",
    "del citations['Year_Citing_Patent-1']\n",
    "\n",
    "citations.rename(columns={\"SPNP_count\": 'SPNP_count_cited_1year_before_citation'}, inplace=True)\n",
    "\n",
    "#### some patents are cited the same year they were filed. For these patents we have no information on their SPNP the year before they were cited. We need to compute their SPNP the moment before they were cited. This is done by multiplying the number of incoming paths of the cited patent by the number of citations received in the year they were filed. This is only an approximation of the number of their outgoing paths, but it is not a bad one because they are unlikely to be cited by other patents granted in the same year that have received citations themselves.\n",
    "\n",
    "citations_same_year_ind = citations['Year_Citing_Patent']==citations['Year_Cited_Patent']\n",
    "citations_same_year_count = citations[citations_same_year_ind].groupby(['Cited_Patent']).size()\n",
    "citations_same_year_count.name = 'citations_at_zero'\n",
    "citations_same_year = pd.DataFrame(citations_same_year_count)\n",
    "\n",
    "citations_same_year = pd.merge(citations_same_year, \n",
    "                      pd.DataFrame({'count_incoming_paths':G.vs['count_incoming_paths']},\n",
    "                                  index=DF_patents['patent_number']), \n",
    "                      how='left', left_index=True, \n",
    "                      right_index=True\n",
    "                     )\n",
    "\n",
    "citations_same_year['SPNP_at_Year_Cited_Patent'] = (log(citations_same_year['count_incoming_paths']+1) +\n",
    "                                                    log(citations_same_year['citations_at_zero']-1+1)\n",
    "                                                    )\n",
    "\n",
    "citations.ix[citations_same_year_ind, \n",
    "             'SPNP_count_cited_1year_before_citation'] = citations_same_year.ix[citations.ix[citations_same_year_ind,\n",
    "                                                                                             'Cited_Patent'],\n",
    "                                                                                'SPNP_at_Year_Cited_Patent'\n",
    "                                                                               ].values\n",
    "del citations_same_year_ind, citations_same_year, citations_same_year_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "citations_grouped_by_citing = citations[['SPNP_count_cited_1year_before_citation',\n",
    "                                          'Citing_Patent']].groupby(['Citing_Patent']).agg(['mean',\n",
    "                                                                                            'std'])\n",
    "\n",
    "DF_patents = pd.merge(DF_patents, citations_grouped_by_citing['SPNP_count_cited_1year_before_citation'], \n",
    "                      how='left', left_on='patent_number', \n",
    "                      right_index=True)\n",
    "\n",
    "DF_patents.rename(columns={\"mean\": 'meanSPNPcited_1year_before',\n",
    "                          \"std\": 'stdSPNPcited_1year_before'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store data\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DF_patents['filing_year'] = DF_patents['filing_year'].astype('uint16')\n",
    "DF_patents['patent_number'] = DF_patents['patent_number'].astype('uint32')\n",
    "\n",
    "for c in DF_patents.columns:\n",
    "#     if c.startswith('SPNP_count'):\n",
    "#         DF_patents[c] = DF_patents[c].fillna(0).astype('uint64')\n",
    "    if DF_patents[c].dtype =='float':\n",
    "        DF_patents[c] = DF_patents[c].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not randomized_control:\n",
    "    DF_patents.to_hdf(data_directory+'centralities/empirical.h5', 'df', complevel=9, complib='blosc')\n",
    "else:\n",
    "    DF_patents.to_hdf(data_directory+'centralities/controls/%s/synthetic_control_%i.h5'%(class_system,\n",
    "                                                                                         randomization_id), 'df', complevel=9, complib='blosc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Total job took: 5474.296887 seconds= 91.238281 minutes\n"
     ]
    }
   ],
   "source": [
    "final_time_length = time.time()-start_time\n",
    "\n",
    "print('Done! Total job took: %f seconds' %final_time_length + '= %f minutes' %(final_time_length/60))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
